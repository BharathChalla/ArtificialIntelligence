{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Actor Critic"
   ],
   "metadata": {
    "id": "gYcrJzoziff9"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Q8U9o4twg-0E"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class CliffWalkingEnv:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.num_of_rows = 6\n",
    "        self.num_of_cols = 10\n",
    "        self.start_state = (5, 0)\n",
    "        self.terminal_state = (5, 9)\n",
    "        self.cliff_states = [(5, i) for i in range(1, 8)]\n",
    "        self.current_state = None\n",
    "        self.directions = [\n",
    "            ((+0, +1), 'up', '⬆'),\n",
    "            ((+1, +0), 'down', '⬇'),\n",
    "            ((+0, -1), 'left', '⬅'),\n",
    "            ((-1, +0), 'right', '➡')\n",
    "        ]\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y = self.current_state\n",
    "        dx, dy = self.directions[action][0]\n",
    "        nx, ny = x + dx, y + dy\n",
    "        next_state = (nx, ny)\n",
    "        if 0 <= nx < self.num_of_rows and 0 <= ny < self.num_of_cols:\n",
    "            self.current_state = next_state\n",
    "\n",
    "        reward = -5.0\n",
    "        is_terminal = False\n",
    "        if self.current_state in self.cliff_states:\n",
    "            reward = -100.0\n",
    "            self.current_state = deepcopy(self.start_state)\n",
    "        elif self.current_state == self.terminal_state:\n",
    "            is_terminal = True\n",
    "        next_state_id = self.cords_to_state_id(self.current_state)\n",
    "        return next_state_id, reward, is_terminal\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.current_state = self.start_state\n",
    "        return self.cords_to_state_id(self.current_state)\n",
    "\n",
    "    def cords_to_state_id(self, state):\n",
    "        r, c = state\n",
    "        return r * self.num_of_cols + c\n",
    "\n",
    "    def print_policy_table(self, actor_model):\n",
    "        print(f'{f\"State/Action\":^12}', end='\\t')\n",
    "        for curr_action in self.directions:\n",
    "            print(f'{curr_action[2]:^10}', end='\\t')\n",
    "        print()\n",
    "\n",
    "        for r in range(0, self.num_of_rows):\n",
    "            for c in range(0, self.num_of_cols):\n",
    "                curr_state_cords = (r, c)\n",
    "                curr_state = self.cords_to_state_id(curr_state_cords)\n",
    "                probs = actor_model(curr_state)\n",
    "                print(f\"{f'({r}, {c})':^10}: \", end='\\t')\n",
    "                for prob in probs:\n",
    "                    print(f\"{prob:^10.3f}\", end='\\t')\n",
    "                print()"
   ],
   "metadata": {
    "id": "nd_TiT7iGduK"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, alpha, in_dims, hl1_dims, hl2_dims, hl3_dims, out_dims):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dims, hl1_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hl1_dims, hl2_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hl2_dims, hl3_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hl3_dims, out_dims),\n",
    "            nn.Softmax(dim=0)\n",
    "        )\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.FloatTensor(np.array([x]))\n",
    "        return self.layers(x)"
   ],
   "metadata": {
    "id": "7I0DMX4ZGeRy"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, alpha1, alpha2, in_dims=1, hl1_dims=256, hl2_dims=512, hl3_dims=128, out_dims=2, gamma=0.7):\n",
    "        self.gamma = gamma\n",
    "        self.actor = NeuralNetwork(alpha1, in_dims, hl1_dims, hl2_dims, hl3_dims, out_dims)\n",
    "        self.critic = NeuralNetwork(alpha2, in_dims, hl1_dims, hl2_dims, hl3_dims, 1)\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.log_probs = None\n",
    "\n",
    "    def next_action(self, curr_state, random=False):\n",
    "        probs = self.actor(curr_state)\n",
    "        if random:\n",
    "            probs = torch.FloatTensor([0.25] * 4)\n",
    "        action_cat_probs = torch.distributions.Categorical(probs)\n",
    "        action = action_cat_probs.sample()\n",
    "        self.log_probs = action_cat_probs.log_prob(action)\n",
    "        return action.item()\n",
    "\n",
    "    def train(self, curr_state, reward, next_state, done):\n",
    "        self.actor.optimizer.zero_grad()\n",
    "        self.critic.optimizer.zero_grad()\n",
    "\n",
    "        critic_curr = self.critic.forward(curr_state)\n",
    "        critic_next = self.critic.forward(next_state)\n",
    "\n",
    "        reward = torch.tensor(reward, dtype=torch.float).to(self.device)\n",
    "        delta = reward + self.gamma * critic_next * (1 - int(done)) - critic_curr\n",
    "\n",
    "        critic_loss = delta ** 2\n",
    "        actor_loss = -self.log_probs * delta\n",
    "\n",
    "        (actor_loss + critic_loss).backward()\n",
    "        self.actor.optimizer.step()\n",
    "        self.critic.optimizer.step()\n",
    "\n",
    "        return actor_loss.item(), critic_loss.item()"
   ],
   "metadata": {
    "id": "_hxmW8IeGecr"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "env = CliffWalkingEnv()\n",
    "num_episodes = 100\n",
    "agent = Agent(alpha1=0.01, alpha2=0.01, in_dims=1, hl1_dims=256, hl2_dims=512, hl3_dims=128, out_dims=4, gamma=0.7)\n",
    "loss_per_episode = []\n",
    "eps_init = 0.9\n",
    "epsilon = 1.0\n",
    "reward_per_episode = []\n",
    "\n",
    "print(f\"Initial Policy Table\")\n",
    "env.print_policy_table(agent.actor)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P087MK0jGelT",
    "outputId": "b9bdf090-bc21-4fa3-f9f2-74a06d185179"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initial Policy Table\n",
      "State/Action\t    ⬆     \t    ⬇     \t    ⬅     \t    ➡     \t\n",
      "  (0, 0)  : \t  0.239   \t  0.259   \t  0.260   \t  0.242   \t\n",
      "  (0, 1)  : \t  0.245   \t  0.254   \t  0.264   \t  0.237   \t\n",
      "  (0, 2)  : \t  0.252   \t  0.248   \t  0.270   \t  0.230   \t\n",
      "  (0, 3)  : \t  0.251   \t  0.248   \t  0.276   \t  0.224   \t\n",
      "  (0, 4)  : \t  0.250   \t  0.250   \t  0.280   \t  0.220   \t\n",
      "  (0, 5)  : \t  0.247   \t  0.252   \t  0.285   \t  0.216   \t\n",
      "  (0, 6)  : \t  0.245   \t  0.253   \t  0.291   \t  0.211   \t\n",
      "  (0, 7)  : \t  0.242   \t  0.254   \t  0.296   \t  0.208   \t\n",
      "  (0, 8)  : \t  0.239   \t  0.256   \t  0.301   \t  0.204   \t\n",
      "  (0, 9)  : \t  0.236   \t  0.258   \t  0.306   \t  0.200   \t\n",
      "  (1, 0)  : \t  0.232   \t  0.259   \t  0.311   \t  0.197   \t\n",
      "  (1, 1)  : \t  0.229   \t  0.261   \t  0.317   \t  0.193   \t\n",
      "  (1, 2)  : \t  0.225   \t  0.262   \t  0.323   \t  0.189   \t\n",
      "  (1, 3)  : \t  0.222   \t  0.263   \t  0.329   \t  0.185   \t\n",
      "  (1, 4)  : \t  0.219   \t  0.264   \t  0.335   \t  0.181   \t\n",
      "  (1, 5)  : \t  0.215   \t  0.265   \t  0.341   \t  0.178   \t\n",
      "  (1, 6)  : \t  0.212   \t  0.267   \t  0.348   \t  0.174   \t\n",
      "  (1, 7)  : \t  0.209   \t  0.267   \t  0.354   \t  0.170   \t\n",
      "  (1, 8)  : \t  0.205   \t  0.268   \t  0.360   \t  0.166   \t\n",
      "  (1, 9)  : \t  0.202   \t  0.269   \t  0.366   \t  0.162   \t\n",
      "  (2, 0)  : \t  0.199   \t  0.270   \t  0.373   \t  0.159   \t\n",
      "  (2, 1)  : \t  0.195   \t  0.271   \t  0.379   \t  0.155   \t\n",
      "  (2, 2)  : \t  0.192   \t  0.271   \t  0.385   \t  0.152   \t\n",
      "  (2, 3)  : \t  0.189   \t  0.272   \t  0.391   \t  0.148   \t\n",
      "  (2, 4)  : \t  0.185   \t  0.273   \t  0.398   \t  0.145   \t\n",
      "  (2, 5)  : \t  0.182   \t  0.273   \t  0.404   \t  0.141   \t\n",
      "  (2, 6)  : \t  0.179   \t  0.274   \t  0.410   \t  0.138   \t\n",
      "  (2, 7)  : \t  0.175   \t  0.274   \t  0.416   \t  0.134   \t\n",
      "  (2, 8)  : \t  0.172   \t  0.275   \t  0.422   \t  0.131   \t\n",
      "  (2, 9)  : \t  0.169   \t  0.275   \t  0.428   \t  0.128   \t\n",
      "  (3, 0)  : \t  0.165   \t  0.275   \t  0.435   \t  0.125   \t\n",
      "  (3, 1)  : \t  0.162   \t  0.275   \t  0.441   \t  0.122   \t\n",
      "  (3, 2)  : \t  0.159   \t  0.276   \t  0.447   \t  0.119   \t\n",
      "  (3, 3)  : \t  0.156   \t  0.276   \t  0.453   \t  0.116   \t\n",
      "  (3, 4)  : \t  0.152   \t  0.276   \t  0.459   \t  0.113   \t\n",
      "  (3, 5)  : \t  0.149   \t  0.276   \t  0.465   \t  0.110   \t\n",
      "  (3, 6)  : \t  0.146   \t  0.275   \t  0.471   \t  0.107   \t\n",
      "  (3, 7)  : \t  0.143   \t  0.275   \t  0.478   \t  0.104   \t\n",
      "  (3, 8)  : \t  0.140   \t  0.275   \t  0.484   \t  0.101   \t\n",
      "  (3, 9)  : \t  0.137   \t  0.275   \t  0.490   \t  0.099   \t\n",
      "  (4, 0)  : \t  0.134   \t  0.274   \t  0.495   \t  0.096   \t\n",
      "  (4, 1)  : \t  0.131   \t  0.274   \t  0.501   \t  0.093   \t\n",
      "  (4, 2)  : \t  0.128   \t  0.274   \t  0.507   \t  0.091   \t\n",
      "  (4, 3)  : \t  0.125   \t  0.273   \t  0.513   \t  0.088   \t\n",
      "  (4, 4)  : \t  0.122   \t  0.273   \t  0.519   \t  0.086   \t\n",
      "  (4, 5)  : \t  0.120   \t  0.272   \t  0.525   \t  0.084   \t\n",
      "  (4, 6)  : \t  0.117   \t  0.271   \t  0.531   \t  0.081   \t\n",
      "  (4, 7)  : \t  0.114   \t  0.271   \t  0.536   \t  0.079   \t\n",
      "  (4, 8)  : \t  0.112   \t  0.270   \t  0.542   \t  0.077   \t\n",
      "  (4, 9)  : \t  0.109   \t  0.269   \t  0.548   \t  0.074   \t\n",
      "  (5, 0)  : \t  0.106   \t  0.268   \t  0.553   \t  0.072   \t\n",
      "  (5, 1)  : \t  0.104   \t  0.267   \t  0.559   \t  0.070   \t\n",
      "  (5, 2)  : \t  0.101   \t  0.266   \t  0.564   \t  0.068   \t\n",
      "  (5, 3)  : \t  0.099   \t  0.265   \t  0.570   \t  0.066   \t\n",
      "  (5, 4)  : \t  0.096   \t  0.264   \t  0.575   \t  0.064   \t\n",
      "  (5, 5)  : \t  0.094   \t  0.263   \t  0.580   \t  0.062   \t\n",
      "  (5, 6)  : \t  0.092   \t  0.262   \t  0.586   \t  0.060   \t\n",
      "  (5, 7)  : \t  0.090   \t  0.261   \t  0.591   \t  0.059   \t\n",
      "  (5, 8)  : \t  0.087   \t  0.260   \t  0.596   \t  0.057   \t\n",
      "  (5, 9)  : \t  0.085   \t  0.259   \t  0.601   \t  0.055   \t\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for i in range(num_episodes):\n",
    "    if i % 10 == 0:\n",
    "      print(f\"Iteration: {i}\")\n",
    "    episode_reward = 0\n",
    "    terminal = False\n",
    "    curr_state = env.reset_state()\n",
    "    actor_loss, critic_loss = 0, 0\n",
    "\n",
    "    if epsilon > 0.1:\n",
    "        epsilon = eps_init ** (i + 1)\n",
    "    while not terminal:\n",
    "        rand_num = np.random.random(1)[0]\n",
    "        random = True\n",
    "        # if rand_num > epsilon:\n",
    "        #     random = False\n",
    "        action = agent.next_action(curr_state, random=random)\n",
    "\n",
    "        next_state, reward, terminal = env.step(action)\n",
    "        a_loss, c_loss = agent.train(curr_state, reward, next_state, terminal)\n",
    "        actor_loss += a_loss\n",
    "        critic_loss += c_loss\n",
    "        episode_reward += reward\n",
    "        curr_state = next_state\n",
    "\n",
    "    reward_per_episode.append(episode_reward)\n",
    "    loss_per_episode.append((actor_loss, critic_loss))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FFXJ_zr_Gzx6",
    "outputId": "d092b970-7a3f-418e-c61c-2607cb563c8f"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration: 0\n",
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "Iteration: 60\n",
      "Iteration: 70\n",
      "Iteration: 80\n",
      "Iteration: 90\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"Final Policy Table\")\n",
    "env.print_policy_table(agent.actor)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OGWXB-LQG21r",
    "outputId": "a00ebaa5-8214-4c6a-d67b-6ebb734ed15f"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Final Policy Table\n",
      "State/Action\t    ⬆     \t    ⬇     \t    ⬅     \t    ➡     \t\n",
      "  (0, 0)  : \t  0.239   \t  0.259   \t  0.260   \t  0.242   \t\n",
      "  (0, 1)  : \t  0.245   \t  0.254   \t  0.264   \t  0.237   \t\n",
      "  (0, 2)  : \t  0.252   \t  0.248   \t  0.270   \t  0.230   \t\n",
      "  (0, 3)  : \t  0.251   \t  0.248   \t  0.276   \t  0.224   \t\n",
      "  (0, 4)  : \t  0.250   \t  0.250   \t  0.280   \t  0.220   \t\n",
      "  (0, 5)  : \t  0.247   \t  0.252   \t  0.285   \t  0.216   \t\n",
      "  (0, 6)  : \t  0.245   \t  0.253   \t  0.291   \t  0.211   \t\n",
      "  (0, 7)  : \t  0.242   \t  0.254   \t  0.296   \t  0.208   \t\n",
      "  (0, 8)  : \t  0.239   \t  0.256   \t  0.301   \t  0.204   \t\n",
      "  (0, 9)  : \t  0.236   \t  0.258   \t  0.306   \t  0.200   \t\n",
      "  (1, 0)  : \t  0.232   \t  0.259   \t  0.311   \t  0.197   \t\n",
      "  (1, 1)  : \t  0.229   \t  0.261   \t  0.317   \t  0.193   \t\n",
      "  (1, 2)  : \t  0.225   \t  0.262   \t  0.323   \t  0.189   \t\n",
      "  (1, 3)  : \t  0.222   \t  0.263   \t  0.329   \t  0.185   \t\n",
      "  (1, 4)  : \t  0.219   \t  0.264   \t  0.335   \t  0.181   \t\n",
      "  (1, 5)  : \t  0.215   \t  0.265   \t  0.341   \t  0.178   \t\n",
      "  (1, 6)  : \t  0.212   \t  0.267   \t  0.348   \t  0.174   \t\n",
      "  (1, 7)  : \t  0.209   \t  0.267   \t  0.354   \t  0.170   \t\n",
      "  (1, 8)  : \t  0.205   \t  0.268   \t  0.360   \t  0.166   \t\n",
      "  (1, 9)  : \t  0.202   \t  0.269   \t  0.366   \t  0.162   \t\n",
      "  (2, 0)  : \t  0.199   \t  0.270   \t  0.373   \t  0.159   \t\n",
      "  (2, 1)  : \t  0.195   \t  0.271   \t  0.379   \t  0.155   \t\n",
      "  (2, 2)  : \t  0.192   \t  0.271   \t  0.385   \t  0.152   \t\n",
      "  (2, 3)  : \t  0.189   \t  0.272   \t  0.391   \t  0.148   \t\n",
      "  (2, 4)  : \t  0.185   \t  0.273   \t  0.398   \t  0.145   \t\n",
      "  (2, 5)  : \t  0.182   \t  0.273   \t  0.404   \t  0.141   \t\n",
      "  (2, 6)  : \t  0.179   \t  0.274   \t  0.410   \t  0.138   \t\n",
      "  (2, 7)  : \t  0.175   \t  0.274   \t  0.416   \t  0.134   \t\n",
      "  (2, 8)  : \t  0.172   \t  0.275   \t  0.422   \t  0.131   \t\n",
      "  (2, 9)  : \t  0.169   \t  0.275   \t  0.428   \t  0.128   \t\n",
      "  (3, 0)  : \t  0.165   \t  0.275   \t  0.435   \t  0.125   \t\n",
      "  (3, 1)  : \t  0.162   \t  0.275   \t  0.441   \t  0.122   \t\n",
      "  (3, 2)  : \t  0.159   \t  0.276   \t  0.447   \t  0.119   \t\n",
      "  (3, 3)  : \t  0.156   \t  0.276   \t  0.453   \t  0.116   \t\n",
      "  (3, 4)  : \t  0.152   \t  0.276   \t  0.459   \t  0.113   \t\n",
      "  (3, 5)  : \t  0.149   \t  0.276   \t  0.465   \t  0.110   \t\n",
      "  (3, 6)  : \t  0.146   \t  0.275   \t  0.471   \t  0.107   \t\n",
      "  (3, 7)  : \t  0.143   \t  0.275   \t  0.478   \t  0.104   \t\n",
      "  (3, 8)  : \t  0.140   \t  0.275   \t  0.484   \t  0.101   \t\n",
      "  (3, 9)  : \t  0.137   \t  0.275   \t  0.490   \t  0.099   \t\n",
      "  (4, 0)  : \t  0.134   \t  0.274   \t  0.495   \t  0.096   \t\n",
      "  (4, 1)  : \t  0.131   \t  0.274   \t  0.501   \t  0.093   \t\n",
      "  (4, 2)  : \t  0.128   \t  0.274   \t  0.507   \t  0.091   \t\n",
      "  (4, 3)  : \t  0.125   \t  0.273   \t  0.513   \t  0.088   \t\n",
      "  (4, 4)  : \t  0.122   \t  0.273   \t  0.519   \t  0.086   \t\n",
      "  (4, 5)  : \t  0.120   \t  0.272   \t  0.525   \t  0.084   \t\n",
      "  (4, 6)  : \t  0.117   \t  0.271   \t  0.531   \t  0.081   \t\n",
      "  (4, 7)  : \t  0.114   \t  0.271   \t  0.536   \t  0.079   \t\n",
      "  (4, 8)  : \t  0.112   \t  0.270   \t  0.542   \t  0.077   \t\n",
      "  (4, 9)  : \t  0.109   \t  0.269   \t  0.548   \t  0.074   \t\n",
      "  (5, 0)  : \t  0.106   \t  0.268   \t  0.553   \t  0.072   \t\n",
      "  (5, 1)  : \t  0.104   \t  0.267   \t  0.559   \t  0.070   \t\n",
      "  (5, 2)  : \t  0.101   \t  0.266   \t  0.564   \t  0.068   \t\n",
      "  (5, 3)  : \t  0.099   \t  0.265   \t  0.570   \t  0.066   \t\n",
      "  (5, 4)  : \t  0.096   \t  0.264   \t  0.575   \t  0.064   \t\n",
      "  (5, 5)  : \t  0.094   \t  0.263   \t  0.580   \t  0.062   \t\n",
      "  (5, 6)  : \t  0.092   \t  0.262   \t  0.586   \t  0.060   \t\n",
      "  (5, 7)  : \t  0.090   \t  0.261   \t  0.591   \t  0.059   \t\n",
      "  (5, 8)  : \t  0.087   \t  0.260   \t  0.596   \t  0.057   \t\n",
      "  (5, 9)  : \t  0.085   \t  0.259   \t  0.601   \t  0.055   \t\n"
     ]
    }
   ]
  }
 ]
}
