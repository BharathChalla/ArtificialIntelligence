{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Actor Critic"
   ],
   "metadata": {
    "id": "gYcrJzoziff9"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Q8U9o4twg-0E"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class CliffWalkingEnv:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.num_of_rows = 6\n",
    "        self.num_of_cols = 10\n",
    "        self.start_state = (5, 0)\n",
    "        self.terminal_state = (5, 9)\n",
    "        self.cliff_states = [(5, i) for i in range(1, 8)]\n",
    "        self.current_state = None\n",
    "        self.directions = [\n",
    "            ((+0, +1), 'up', '⬆'),\n",
    "            ((+1, +0), 'down', '⬇'),\n",
    "            ((+0, -1), 'left', '⬅'),\n",
    "            ((-1, +0), 'right', '➡')\n",
    "        ]\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y = self.current_state\n",
    "        dx, dy = self.directions[action][0]\n",
    "        nx, ny = x + dx, y + dy\n",
    "        next_state = (nx, ny)\n",
    "        if 0 <= nx < self.num_of_rows and 0 <= ny < self.num_of_cols:\n",
    "            self.current_state = next_state\n",
    "\n",
    "        reward = -5.0\n",
    "        is_terminal = False\n",
    "        if self.current_state in self.cliff_states:\n",
    "            reward = -100.0\n",
    "            self.current_state = deepcopy(self.start_state)\n",
    "        elif self.current_state == self.terminal_state:\n",
    "            is_terminal = True\n",
    "        next_state_id = self.cords_to_state_id(self.current_state)\n",
    "        return next_state_id, reward, is_terminal\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.current_state = self.start_state\n",
    "        return self.cords_to_state_id(self.current_state)\n",
    "\n",
    "    def cords_to_state_id(self, state):\n",
    "        r, c = state\n",
    "        return r * self.num_of_cols + c\n",
    "\n",
    "    def print_policy_table(self, actor_model):\n",
    "        print(f'{f\"State/Action\":^12}', end='\\t')\n",
    "        for curr_action in self.directions:\n",
    "            print(f'{curr_action[2]:^10}', end='\\t')\n",
    "        print()\n",
    "\n",
    "        for r in range(0, self.num_of_rows):\n",
    "            for c in range(0, self.num_of_cols):\n",
    "                curr_state_cords = (r, c)\n",
    "                curr_state = self.cords_to_state_id(curr_state_cords)\n",
    "                probs = actor_model(curr_state)\n",
    "                print(f\"{f'({r}, {c})':^10}: \", end='\\t')\n",
    "                for prob in probs:\n",
    "                    print(f\"{prob:^10.3f}\", end='\\t')\n",
    "                print()"
   ],
   "metadata": {
    "id": "nd_TiT7iGduK"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, alpha, in_dims, hl1_dims, hl2_dims, hl3_dims, out_dims):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dims, hl1_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hl1_dims, hl2_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hl2_dims, hl3_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hl3_dims, out_dims),\n",
    "            nn.Softmax(dim=0)\n",
    "        )\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.FloatTensor(np.array([x]))\n",
    "        return self.layers(x)"
   ],
   "metadata": {
    "id": "7I0DMX4ZGeRy"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, alpha1, alpha2, in_dims=1, hl1_dims=256, hl2_dims=512, hl3_dims=128, out_dims=2, gamma=0.7):\n",
    "        self.gamma = gamma\n",
    "        self.actor = NeuralNetwork(alpha1, in_dims, hl1_dims, hl2_dims, hl3_dims, out_dims)\n",
    "        self.critic = NeuralNetwork(alpha2, in_dims, hl1_dims, hl2_dims, hl3_dims, 1)\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.log_probs = None\n",
    "\n",
    "    def next_action(self, curr_state, random=False):\n",
    "        probs = self.actor(curr_state)\n",
    "        if random:\n",
    "            probs = torch.FloatTensor([0.25] * 4)\n",
    "        action_cat_probs = torch.distributions.Categorical(probs)\n",
    "        action = action_cat_probs.sample()\n",
    "        self.log_probs = action_cat_probs.log_prob(action)\n",
    "        return action.item()\n",
    "\n",
    "    def train(self, curr_state, reward, next_state, done):\n",
    "        self.actor.optimizer.zero_grad()\n",
    "        self.critic.optimizer.zero_grad()\n",
    "\n",
    "        critic_curr = self.critic.forward(curr_state)\n",
    "        critic_next = self.critic.forward(next_state)\n",
    "\n",
    "        reward = torch.tensor(reward, dtype=torch.float).to(self.device)\n",
    "        delta = reward + self.gamma * critic_next * (1 - int(done)) - critic_curr\n",
    "\n",
    "        critic_loss = delta ** 2\n",
    "        actor_loss = -self.log_probs * delta\n",
    "\n",
    "        (actor_loss + critic_loss).backward()\n",
    "        self.actor.optimizer.step()\n",
    "        self.critic.optimizer.step()\n",
    "\n",
    "        return actor_loss.item(), critic_loss.item()"
   ],
   "metadata": {
    "id": "_hxmW8IeGecr"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "env = CliffWalkingEnv()\n",
    "num_episodes = 25\n",
    "agent = Agent(alpha1=0.01, alpha2=0.01, in_dims=1, hl1_dims=256, hl2_dims=512, hl3_dims=128, out_dims=4, gamma=0.7)\n",
    "loss_per_episode = []\n",
    "eps_init = 0.9\n",
    "epsilon = 1.0\n",
    "reward_per_episode = []\n",
    "\n",
    "print(f\"Initial Policy Table\")\n",
    "env.print_policy_table(agent.actor)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P087MK0jGelT",
    "outputId": "19e74a12-ede8-4213-c0aa-7a7a6b09e7fe"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initial Policy Table\n",
      "State/Action\t    ⬆     \t    ⬇     \t    ⬅     \t    ➡     \t\n",
      "  (0, 0)  : \t  0.241   \t  0.214   \t  0.283   \t  0.262   \t\n",
      "  (0, 1)  : \t  0.249   \t  0.209   \t  0.269   \t  0.273   \t\n",
      "  (0, 2)  : \t  0.256   \t  0.206   \t  0.253   \t  0.285   \t\n",
      "  (0, 3)  : \t  0.262   \t  0.204   \t  0.239   \t  0.295   \t\n",
      "  (0, 4)  : \t  0.266   \t  0.204   \t  0.228   \t  0.302   \t\n",
      "  (0, 5)  : \t  0.270   \t  0.204   \t  0.218   \t  0.309   \t\n",
      "  (0, 6)  : \t  0.273   \t  0.204   \t  0.207   \t  0.316   \t\n",
      "  (0, 7)  : \t  0.276   \t  0.203   \t  0.198   \t  0.324   \t\n",
      "  (0, 8)  : \t  0.279   \t  0.201   \t  0.188   \t  0.331   \t\n",
      "  (0, 9)  : \t  0.282   \t  0.200   \t  0.179   \t  0.339   \t\n",
      "  (1, 0)  : \t  0.285   \t  0.197   \t  0.170   \t  0.348   \t\n",
      "  (1, 1)  : \t  0.288   \t  0.194   \t  0.162   \t  0.356   \t\n",
      "  (1, 2)  : \t  0.291   \t  0.191   \t  0.153   \t  0.365   \t\n",
      "  (1, 3)  : \t  0.293   \t  0.188   \t  0.146   \t  0.373   \t\n",
      "  (1, 4)  : \t  0.296   \t  0.185   \t  0.138   \t  0.381   \t\n",
      "  (1, 5)  : \t  0.298   \t  0.182   \t  0.131   \t  0.389   \t\n",
      "  (1, 6)  : \t  0.300   \t  0.179   \t  0.124   \t  0.397   \t\n",
      "  (1, 7)  : \t  0.302   \t  0.175   \t  0.117   \t  0.406   \t\n",
      "  (1, 8)  : \t  0.303   \t  0.172   \t  0.111   \t  0.414   \t\n",
      "  (1, 9)  : \t  0.305   \t  0.169   \t  0.105   \t  0.421   \t\n",
      "  (2, 0)  : \t  0.306   \t  0.165   \t  0.099   \t  0.429   \t\n",
      "  (2, 1)  : \t  0.307   \t  0.162   \t  0.094   \t  0.437   \t\n",
      "  (2, 2)  : \t  0.309   \t  0.158   \t  0.089   \t  0.444   \t\n",
      "  (2, 3)  : \t  0.310   \t  0.155   \t  0.084   \t  0.452   \t\n",
      "  (2, 4)  : \t  0.310   \t  0.152   \t  0.079   \t  0.459   \t\n",
      "  (2, 5)  : \t  0.311   \t  0.148   \t  0.074   \t  0.467   \t\n",
      "  (2, 6)  : \t  0.312   \t  0.145   \t  0.070   \t  0.474   \t\n",
      "  (2, 7)  : \t  0.312   \t  0.141   \t  0.066   \t  0.481   \t\n",
      "  (2, 8)  : \t  0.312   \t  0.138   \t  0.062   \t  0.488   \t\n",
      "  (2, 9)  : \t  0.312   \t  0.134   \t  0.059   \t  0.495   \t\n",
      "  (3, 0)  : \t  0.312   \t  0.131   \t  0.055   \t  0.502   \t\n",
      "  (3, 1)  : \t  0.312   \t  0.128   \t  0.052   \t  0.508   \t\n",
      "  (3, 2)  : \t  0.312   \t  0.124   \t  0.049   \t  0.515   \t\n",
      "  (3, 3)  : \t  0.312   \t  0.121   \t  0.046   \t  0.521   \t\n",
      "  (3, 4)  : \t  0.311   \t  0.118   \t  0.043   \t  0.528   \t\n",
      "  (3, 5)  : \t  0.311   \t  0.115   \t  0.040   \t  0.534   \t\n",
      "  (3, 6)  : \t  0.310   \t  0.112   \t  0.038   \t  0.540   \t\n",
      "  (3, 7)  : \t  0.309   \t  0.109   \t  0.036   \t  0.546   \t\n",
      "  (3, 8)  : \t  0.309   \t  0.106   \t  0.034   \t  0.552   \t\n",
      "  (3, 9)  : \t  0.308   \t  0.103   \t  0.031   \t  0.558   \t\n",
      "  (4, 0)  : \t  0.307   \t  0.100   \t  0.030   \t  0.564   \t\n",
      "  (4, 1)  : \t  0.305   \t  0.097   \t  0.028   \t  0.570   \t\n",
      "  (4, 2)  : \t  0.304   \t  0.094   \t  0.026   \t  0.575   \t\n",
      "  (4, 3)  : \t  0.303   \t  0.092   \t  0.024   \t  0.581   \t\n",
      "  (4, 4)  : \t  0.302   \t  0.089   \t  0.023   \t  0.587   \t\n",
      "  (4, 5)  : \t  0.300   \t  0.087   \t  0.021   \t  0.592   \t\n",
      "  (4, 6)  : \t  0.299   \t  0.084   \t  0.020   \t  0.597   \t\n",
      "  (4, 7)  : \t  0.297   \t  0.082   \t  0.019   \t  0.603   \t\n",
      "  (4, 8)  : \t  0.295   \t  0.079   \t  0.018   \t  0.608   \t\n",
      "  (4, 9)  : \t  0.294   \t  0.077   \t  0.017   \t  0.613   \t\n",
      "  (5, 0)  : \t  0.292   \t  0.074   \t  0.015   \t  0.618   \t\n",
      "  (5, 1)  : \t  0.290   \t  0.072   \t  0.014   \t  0.623   \t\n",
      "  (5, 2)  : \t  0.288   \t  0.070   \t  0.014   \t  0.628   \t\n",
      "  (5, 3)  : \t  0.287   \t  0.068   \t  0.013   \t  0.633   \t\n",
      "  (5, 4)  : \t  0.285   \t  0.066   \t  0.012   \t  0.638   \t\n",
      "  (5, 5)  : \t  0.283   \t  0.064   \t  0.011   \t  0.642   \t\n",
      "  (5, 6)  : \t  0.281   \t  0.062   \t  0.010   \t  0.647   \t\n",
      "  (5, 7)  : \t  0.279   \t  0.060   \t  0.010   \t  0.651   \t\n",
      "  (5, 8)  : \t  0.277   \t  0.058   \t  0.009   \t  0.656   \t\n",
      "  (5, 9)  : \t  0.275   \t  0.056   \t  0.009   \t  0.660   \t\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for i in range(num_episodes):\n",
    "    print(f\"Iteration: {i}\")\n",
    "    episode_reward = 0\n",
    "    terminal = False\n",
    "    curr_state = env.reset_state()\n",
    "    actor_loss, critic_loss = 0, 0\n",
    "\n",
    "    if epsilon > 0.1:\n",
    "        epsilon = eps_init ** (i + 1)\n",
    "    while not terminal:\n",
    "        rand_num = np.random.random(1)[0]\n",
    "        random = True\n",
    "        # if rand_num > epsilon:\n",
    "        #     random = False\n",
    "        action = agent.next_action(curr_state, random=random)\n",
    "\n",
    "        next_state, reward, terminal = env.step(action)\n",
    "        a_loss, c_loss = agent.train(curr_state, reward, next_state, terminal)\n",
    "        actor_loss += a_loss\n",
    "        critic_loss += c_loss\n",
    "        episode_reward += reward\n",
    "        curr_state = next_state\n",
    "\n",
    "    reward_per_episode.append(episode_reward)\n",
    "    loss_per_episode.append((actor_loss, critic_loss))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FFXJ_zr_Gzx6",
    "outputId": "2fe347e3-1b63-4aab-fe18-4b8cb5262ef1"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration: 0\n",
      "Iteration: 1\n",
      "Iteration: 2\n",
      "Iteration: 3\n",
      "Iteration: 4\n",
      "Iteration: 5\n",
      "Iteration: 6\n",
      "Iteration: 7\n",
      "Iteration: 8\n",
      "Iteration: 9\n",
      "Iteration: 10\n",
      "Iteration: 11\n",
      "Iteration: 12\n",
      "Iteration: 13\n",
      "Iteration: 14\n",
      "Iteration: 15\n",
      "Iteration: 16\n",
      "Iteration: 17\n",
      "Iteration: 18\n",
      "Iteration: 19\n",
      "Iteration: 20\n",
      "Iteration: 21\n",
      "Iteration: 22\n",
      "Iteration: 23\n",
      "Iteration: 24\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"Final Policy Table\")\n",
    "env.print_policy_table(agent.actor)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OGWXB-LQG21r",
    "outputId": "63149b6a-461c-4101-e7fb-b049146125fa"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Final Policy Table\n",
      "State/Action\t    ⬆     \t    ⬇     \t    ⬅     \t    ➡     \t\n",
      "  (0, 0)  : \t  0.241   \t  0.214   \t  0.283   \t  0.262   \t\n",
      "  (0, 1)  : \t  0.249   \t  0.209   \t  0.269   \t  0.273   \t\n",
      "  (0, 2)  : \t  0.256   \t  0.206   \t  0.253   \t  0.285   \t\n",
      "  (0, 3)  : \t  0.262   \t  0.204   \t  0.239   \t  0.295   \t\n",
      "  (0, 4)  : \t  0.266   \t  0.204   \t  0.228   \t  0.302   \t\n",
      "  (0, 5)  : \t  0.270   \t  0.204   \t  0.218   \t  0.309   \t\n",
      "  (0, 6)  : \t  0.273   \t  0.204   \t  0.207   \t  0.316   \t\n",
      "  (0, 7)  : \t  0.276   \t  0.203   \t  0.198   \t  0.324   \t\n",
      "  (0, 8)  : \t  0.279   \t  0.201   \t  0.188   \t  0.331   \t\n",
      "  (0, 9)  : \t  0.282   \t  0.200   \t  0.179   \t  0.339   \t\n",
      "  (1, 0)  : \t  0.285   \t  0.197   \t  0.170   \t  0.348   \t\n",
      "  (1, 1)  : \t  0.288   \t  0.194   \t  0.162   \t  0.356   \t\n",
      "  (1, 2)  : \t  0.291   \t  0.191   \t  0.153   \t  0.365   \t\n",
      "  (1, 3)  : \t  0.293   \t  0.188   \t  0.146   \t  0.373   \t\n",
      "  (1, 4)  : \t  0.296   \t  0.185   \t  0.138   \t  0.381   \t\n",
      "  (1, 5)  : \t  0.298   \t  0.182   \t  0.131   \t  0.389   \t\n",
      "  (1, 6)  : \t  0.300   \t  0.179   \t  0.124   \t  0.397   \t\n",
      "  (1, 7)  : \t  0.302   \t  0.175   \t  0.117   \t  0.406   \t\n",
      "  (1, 8)  : \t  0.303   \t  0.172   \t  0.111   \t  0.414   \t\n",
      "  (1, 9)  : \t  0.305   \t  0.169   \t  0.105   \t  0.421   \t\n",
      "  (2, 0)  : \t  0.306   \t  0.165   \t  0.099   \t  0.429   \t\n",
      "  (2, 1)  : \t  0.307   \t  0.162   \t  0.094   \t  0.437   \t\n",
      "  (2, 2)  : \t  0.309   \t  0.158   \t  0.089   \t  0.444   \t\n",
      "  (2, 3)  : \t  0.310   \t  0.155   \t  0.084   \t  0.452   \t\n",
      "  (2, 4)  : \t  0.310   \t  0.152   \t  0.079   \t  0.459   \t\n",
      "  (2, 5)  : \t  0.311   \t  0.148   \t  0.074   \t  0.467   \t\n",
      "  (2, 6)  : \t  0.312   \t  0.145   \t  0.070   \t  0.474   \t\n",
      "  (2, 7)  : \t  0.312   \t  0.141   \t  0.066   \t  0.481   \t\n",
      "  (2, 8)  : \t  0.312   \t  0.138   \t  0.062   \t  0.488   \t\n",
      "  (2, 9)  : \t  0.312   \t  0.134   \t  0.059   \t  0.495   \t\n",
      "  (3, 0)  : \t  0.312   \t  0.131   \t  0.055   \t  0.502   \t\n",
      "  (3, 1)  : \t  0.312   \t  0.128   \t  0.052   \t  0.508   \t\n",
      "  (3, 2)  : \t  0.312   \t  0.124   \t  0.049   \t  0.515   \t\n",
      "  (3, 3)  : \t  0.312   \t  0.121   \t  0.046   \t  0.521   \t\n",
      "  (3, 4)  : \t  0.311   \t  0.118   \t  0.043   \t  0.528   \t\n",
      "  (3, 5)  : \t  0.311   \t  0.115   \t  0.040   \t  0.534   \t\n",
      "  (3, 6)  : \t  0.310   \t  0.112   \t  0.038   \t  0.540   \t\n",
      "  (3, 7)  : \t  0.309   \t  0.109   \t  0.036   \t  0.546   \t\n",
      "  (3, 8)  : \t  0.309   \t  0.106   \t  0.034   \t  0.552   \t\n",
      "  (3, 9)  : \t  0.308   \t  0.103   \t  0.031   \t  0.558   \t\n",
      "  (4, 0)  : \t  0.307   \t  0.100   \t  0.030   \t  0.564   \t\n",
      "  (4, 1)  : \t  0.305   \t  0.097   \t  0.028   \t  0.570   \t\n",
      "  (4, 2)  : \t  0.304   \t  0.094   \t  0.026   \t  0.575   \t\n",
      "  (4, 3)  : \t  0.303   \t  0.092   \t  0.024   \t  0.581   \t\n",
      "  (4, 4)  : \t  0.302   \t  0.089   \t  0.023   \t  0.587   \t\n",
      "  (4, 5)  : \t  0.300   \t  0.087   \t  0.021   \t  0.592   \t\n",
      "  (4, 6)  : \t  0.299   \t  0.084   \t  0.020   \t  0.597   \t\n",
      "  (4, 7)  : \t  0.297   \t  0.082   \t  0.019   \t  0.603   \t\n",
      "  (4, 8)  : \t  0.295   \t  0.079   \t  0.018   \t  0.608   \t\n",
      "  (4, 9)  : \t  0.294   \t  0.077   \t  0.017   \t  0.613   \t\n",
      "  (5, 0)  : \t  0.292   \t  0.074   \t  0.015   \t  0.618   \t\n",
      "  (5, 1)  : \t  0.290   \t  0.072   \t  0.014   \t  0.623   \t\n",
      "  (5, 2)  : \t  0.288   \t  0.070   \t  0.014   \t  0.628   \t\n",
      "  (5, 3)  : \t  0.287   \t  0.068   \t  0.013   \t  0.633   \t\n",
      "  (5, 4)  : \t  0.285   \t  0.066   \t  0.012   \t  0.638   \t\n",
      "  (5, 5)  : \t  0.283   \t  0.064   \t  0.011   \t  0.642   \t\n",
      "  (5, 6)  : \t  0.281   \t  0.062   \t  0.010   \t  0.647   \t\n",
      "  (5, 7)  : \t  0.279   \t  0.060   \t  0.010   \t  0.651   \t\n",
      "  (5, 8)  : \t  0.277   \t  0.058   \t  0.009   \t  0.656   \t\n",
      "  (5, 9)  : \t  0.275   \t  0.056   \t  0.009   \t  0.660   \t\n"
     ]
    }
   ]
  }
 ]
}
